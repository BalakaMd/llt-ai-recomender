Архитектурная спецификация и стратегия реализации микросервиса AI Recommender Service
=====================================================================================

1\. Введение и концептуальные основы системы
--------------------------------------------

Проектирование и разработка интеллектуальных систем поддержки принятия решений в туристической индустрии требует глубокого понимания как алгоритмических основ генеративного искусственного интеллекта, так и принципов построения распределенных высоконагруженных систем. В рамках дипломного проекта «Интеллектуальный мобильный ассистент для планирования путешествий» (LittleLifeTrip), микросервис **AI Recommender Service** занимает центральное место, выполняя роль когнитивного ядра архитектуры. В отличие от детерминированных сервисов, оперирующих жесткой логикой (например, сервиса бронирования или авторизации), данный компонент отвечает за синтез неструктурированных предпочтений пользователя, контекстуальных данных и жестких ограничений в связный, логически обоснованный и персонализированный продукт --- маршрут путешествия.^1^

Анализ архитектурной документации указывает на то, что AI Recommender Service функционирует как внутренний (internal) сервис, изолированный от публичного доступа API Gateway и взаимодействующий исключительно с другими сервисами внутри кластера (преимущественно с Trip Service).^2^ Эта изоляция диктует особые требования к протоколам взаимодействия, безопасности и форматам данных. Сервис должен быть спроектирован как stateless-компонент, не хранящий состояние пользовательской сессии, но обеспечивающий полную идемпотентность и воспроизводимость генераций через механизм логирования в базу данных `integration.ai_runs`.^2^

Современные тенденции в разработке AI-систем смещают фокус с простых вызовов LLM (Large Language Models) на сложные инженерные пайплайны, включающие этапы предварительной обработки данных (grounding), конструирования контекста (prompt engineering) и строгой валидации выходных данных (structured output validation).^3^ В данном отчете представлен исчерпывающий план реализации сервиса, учитывающий необходимость оркестрации вызовов к внешним провайдерам (OpenAI/Gemini) и внутренним источникам данных (Integration Service: Maps, Weather). Особое внимание уделяется применению паттернов проектирования, обеспечивающих надежность и наблюдаемость (observability) системы, что критически важно для дипломной работы уровня Software Engineering.

### 1.1 Роль сервиса в микросервисной экосистеме

Согласно предоставленной диаграмме архитектуры, AI Recommender Service не существует в вакууме. Он является потребителем данных от `Integration Service` (погода, карты) и поставщиком структурированных планов для `Trip Service`. Эта зависимость формирует направленный граф вызовов, где задержка (latency) AI-сервиса становится критическим путем для пользовательского опыта. Учитывая, что генерация ответа LLM может занимать от 10 до 60 секунд, архитектура сервиса должна быть полностью асинхронной, чтобы избежать блокировки потоков обработки запросов и исчерпания пула соединений.^5^

Ниже приведена таблица, характеризующая границы ответственности сервиса в контексте общей системы:

| **Аспект архитектуры** | **Характеристика реализации** | **Обоснование и влияние на разработку** |
| --- | --- | --- |
| **Тип взаимодействия** | Внутренний gRPC или REST (HTTP/2) |

Сервис недоступен извне; требуется аутентификация service-to-service (mTLS/JWT) для предотвращения несанкционированного доступа внутри периметра.^2^

 |
| **Управление состоянием** | Stateless (Без сохранения состояния) | Сервис обрабатывает каждый запрос изолированно. История диалога или контекст модификации маршрута должны передаваться в теле запроса от Trip Service. |
| **Зависимости данных** | Integration Service (Maps/Weather) |

Для минимизации "галлюцинаций" модели, сервис должен сначала запросить фактические данные о погоде и POI, и лишь затем формировать промпт.^7^

 |
| **Персистентность** | PostgreSQL (`integration` schema) |

Асинхронная запись логов генерации (промпт, ответ, токены) в таблицу `integration.ai_runs` для аналитики и отладки, не блокируя основной ответ.^8^

 |
| **Вычислительная модель** | I/O Bound | Основное время работы сервиса --- ожидание ответов от LLM API и Integration Service, что диктует использование `asyncio` и асинхронных драйверов БД. |

2\. Технологический стек и выбор инструментов
---------------------------------------------

Выбор технологического стека для реализации AI Recommender Service обусловлен необходимостью баланса между скоростью разработки (критично для дипломного проекта), производительностью и строгостью типизации данных. Анализ современных практик разработки микросервисов на Python выделяет комбинацию **FastAPI**, **Pydantic V2** и **SQLAlchemy (Async)** как де-факто стандарт для высоконагруженных систем, интегрированных с ML/AI.^5^

### 2.1 Фреймворк и Асинхронность: FastAPI

Использование FastAPI обосновано его нативной поддержкой стандарта ASGI (Asynchronous Server Gateway Interface). В отличие от синхронных фреймворков (Flask, Django), FastAPI позволяет обрабатывать тысячи одновременных соединений в одном процессе, что критически важно для сервиса, который большую часть времени проводит в ожидании ответа от внешних API (OpenAI, Google Maps).^10^ В контексте генерации маршрутов, где время ответа LLM может достигать десятков секунд, синхронный воркер был бы заблокирован, что привело бы к отказу в обслуживании при минимальной нагрузке. Асинхронная модель позволяет переключать контекст выполнения на другие задачи во время I/O операций.

Кроме того, FastAPI обеспечивает автоматическую генерацию документации OpenAPI (Swagger UI), что существенно упрощает интеграцию с Trip Service и тестирование эндпоинтов на ранних этапах разработки.^9^ Декларативное описание параметров запроса и ответа через Pydantic гарантирует, что любые отклонения в структуре данных будут перехвачены на уровне валидации, не доходя до бизнес-логики.

### 2.2 Валидация данных и структурная типизация: Pydantic V2

Работа с LLM сопряжена с высокой степенью неопределенности формата выходных данных. Модели могут возвращать некорректный JSON, пропускать поля или менять типы данных. Pydantic V2 выступает в роли "защитного барьера" (guardrail), обеспечивая строгую валидацию и парсинг ответов LLM. Использование функциональности `Structured Outputs` (или JSON Mode) в сочетании с моделями Pydantic позволяет математически гарантировать соответствие сгенерированного маршрута ожидаемой схеме БД.^13^

Ключевым преимуществом Pydantic является возможность определения сложных вложенных структур (например, список дней, каждый из которых содержит список активностей, каждая из которых имеет координаты и бюджет), что идеально ложится на структуру таблицы `trip.itinerary_items`, описанную в ER-диаграмме.^2^

### 2.3 Взаимодействие с базой данных: SQLAlchemy AsyncIO

Для работы с PostgreSQL в асинхронном режиме необходимо использовать драйвер `asyncpg` в связке с асинхронным расширением SQLAlchemy. Это позволяет выполнять операции записи логов в таблицу `integration.ai_runs` без блокировки основного цикла событий (Event Loop). Таблица `ai_runs` содержит поле `response` типа `JSONB`, что требует от ORM поддержки диалекта PostgreSQL для эффективной сериализации и десериализации JSON-структур.^2^

3\. Проектирование базы данных и схемы данных
---------------------------------------------

Согласно предоставленной ER-диаграмме и архитектурному описанию, сервис AI Recommender владеет таблицей `integration.ai_runs`. Эта таблица служит журналом аудита всех взаимодействий с искусственным интеллектом, что позволяет анализировать качество генерации, затраты на токены и выявлять ошибки.

### 3.1 Детальная спецификация таблицы `integration.ai_runs`

Таблица должна быть реализована с использованием SQLAlchemy моделей. Ниже представлена детальная спецификация полей и их назначение в контексте микросервиса.

| **Поле БД** | **Тип данных (PostgreSQL)** | **Тип данных (Python/SQLAlchemy)** | **Описание и назначение** |
| --- | --- | --- | --- |
| `id` | `UUID` (PK) | `uuid.UUID` | Уникальный идентификатор запуска генерации. Генерируется на стороне приложения или БД. |
| `user_id` | `UUID` (FK) | `uuid.UUID` | Идентификатор пользователя, инициировавшего запрос. Критично для персонализации и rate-limiting. |
| `trip_id` | `UUID` (FK, Nullable) | `uuid.UUID` | Ссылка на созданное путешествие (если применимо). Может быть NULL, если генерация не привела к созданию трипа. |
| `provider` | `ENUM` | `enum.Enum` | Провайдер LLM (например, `openai`, `gemini`, `anthropic`). Важно для аналитики затрат и A/B тестирования моделей. |
| `prompt` | `TEXT` | `str` | Полный текст промпта, отправленного модели, включая системные инструкции и контекст. |
| `response` | `JSONB` | `dict` / `list` | Сырой (или очищенный) JSON ответ модели. Тип `JSONB` позволяет выполнять SQL-запросы по содержимому ответа. |
| `tokens_used` | `INT` | `int` | Количество токенов (input + output), затраченных на операцию. Используется для расчета стоимости. |
| `status` | `VARCHAR` | `str` | Статус выполнения: `pending`, `completed`, `failed`. |
| `created_at` | `TIMESTAMPTZ` | `datetime` | Временная метка создания записи с учетом часового пояса. |

**Анализ связей:** Таблица имеет внешние ключи к `auth.users` и `trip.trips` (согласно ER-диаграмме ^2^). Однако, в микросервисной архитектуре прямые `ForeignKey` ограничения между базами данных разных сервисов (если они разнесены физически) невозможны. В рамках данного проекта (монолитная БД с разделением по схемам) использование `ForeignKey` допустимо, но в коде следует обрабатывать `IntegrityError`.

### 3.2 Стратегия асинхронного логирования

Запись в таблицу `integration.ai_runs` является вспомогательной операцией и не должна увеличивать время ответа пользователю. Для реализации этого требования следует использовать механизм **BackgroundTasks** в FastAPI. Это позволяет вернуть HTTP-ответ (сгенерированный маршрут) пользователю немедленно, а запись логов в БД выполнить в фоновом режиме после завершения запроса.^15^

Пример логики работы с БД:

1.  Сервис получает запрос.

2.  Формирует промпт.

3.  Отправляет запрос в LLM и получает ответ.

4.  Валидирует ответ.

5.  **Добавляет задачу в BackgroundTasks:** `log_ai_run(user_id, prompt, response, tokens)`.

6.  Возвращает ответ клиенту.

7.  FastAPI выполняет функцию `log_ai_run`, которая открывает сессию БД и сохраняет запись.

4\. Моделирование предметной области и Pydantic схемы
-----------------------------------------------------

Для обеспечения надежности взаимодействия между Trip Service и AI Recommender Service необходимо строго определить контракты данных (Data Contracts). В Python экосистеме стандартом является библиотека Pydantic. Схемы Pydantic будут использоваться как для валидации входящих HTTP-запросов, так и для структурирования ответов от LLM.

### 4.1 Входные данные (Request Models)

На основе списка эндпоинтов и бизнес-требований ^2^, основной входной объект для генерации маршрута должен содержать профиль пользователя, параметры поездки и ограничения.

**Схема `RecommendationRequest`:**

Python

```
from pydantic import BaseModel, Field, conint, confloat
from typing import List, Optional
from datetime import date

class UserPreferences(BaseModel):
    interests: List[str] = Field(..., description="Список интересов пользователя, например: ['history', 'food']")
    transport_modes: List[str] = Field(..., description="Предпочитаемые способы передвижения")
    avg_daily_budget: Optional[int] = Field(None, description="Средний дневной бюджет")

class TripConstraints(BaseModel):
    origin_city: str
    destination_city: Optional[str] = None
    start_date: Optional[date]
    end_date: Optional[date]
    duration_days: conint(ge=1, le=15) = Field(..., description="Длительность поездки в днях")
    total_budget: Optional[int]
    travel_party_size: int = Field(1, ge=1)

class RecommendationRequest(BaseModel):
    user_profile: UserPreferences
    constraints: TripConstraints
    timezone: str = "UTC"

```

*Аналитика:* Использование `conint` (constrained int) для `duration_days` критически важно. Генерация детального почасового плана на 30 дней может превысить контекстное окно модели или лимит времени выполнения. Ограничение в 15 дней является разумным компромиссом для MVP.

### 4.2 Выходные данные (Response Models & Structured Output)

Это, пожалуй, самый важный аспект реализации. Ответ сервиса должен быть не просто текстом, а валидной JSON-структурой, которую Trip Service сможет сохранить в таблицы `trip.trips` и `trip.itinerary_items`.^17^

**Схема `ItineraryResponse`:**

Python

```
class GeoCoordinates(BaseModel):
    lat: float
    lng: float

class ItineraryItem(BaseModel):
    day_index: int
    order_index: int
    title: str
    description: str
    place_name: str
    coordinates: Optional[GeoCoordinates]
    estimated_cost: Optional[float]
    duration_minutes: Optional[int]
    rationale: str  # Объяснение, почему это место выбрано (для /explain)

class TripPlan(BaseModel):
    title: str
    summary: str
    total_budget_estimate: float
    currency: str
    itinerary: List[ItineraryItem]
    tags: List[str]  # e.g., "Relaxing", "Cultural"

```

Эта структура напрямую мапится на таблицы `trip.trips` и `trip.itinerary_items`. Поле `rationale` в `ItineraryItem` позволяет реализовать функционал "Explain Plan" без повторного обращения к LLM, просто отображая пользователю сохраненное объяснение.^2^

5\. Интеграция с внешними сервисами и Context Injection
-------------------------------------------------------

Согласно архитектуре, AI Recommender является "оркестратором", который должен собрать контекст перед тем, как задать вопрос LLM. "Галлюцинации" моделей часто возникают из-за отсутствия актуальных данных (например, предложение посетить парк в дождливый день). Решением является паттерн **Retrieval-Augmented Generation (RAG)**, но применительно к API, а не к векторным базам данных.

### 5.1 Клиент интеграции (Internal Integration Client)

Необходимо реализовать сервис-клиент, использующий `httpx` для асинхронных вызовов к соседнему микросервису `Integration Service`.

**Сценарий обогащения контекста:**

1.  **Получение запроса:** Пользователь хочет в Париж на 3 дня.

2.  **Параллельные запросы (Async Gather):**

    -   Запрос к `Integration Service /weather`: Получить прогноз погоды на указанные даты для Парижа.^2^

    -   Запрос к `Integration Service /maps`: Получить список топ-достопримечательностей (опционально, если нужно сузить поиск).

3.  **Формирование контекста:** Результаты этих запросов не возвращаются пользователю напрямую, а внедряются в системный промпт LLM.

Python

```
# Примерная логика (псевдокод)
weather_task = integration_client.get_weather(city, dates)
poi_task = integration_client.search_places(city, interests)
weather, pois = await asyncio.gather(weather_task, poi_task)

system_prompt = f"""
Ты планировщик путешествий.
Контекст погоды: {weather} (Учти это при выборе активностей: музей при дожде, парк при солнце).
Доступные места: {pois}.
"""

```

Эта стратегия, описанная в научных источниках как "grounding" (заземление), значительно повышает фактическую точность и логическую связность маршрута.^4^

6\. Стратегия Промпт-инжиниринга и Взаимодействие с LLM
-------------------------------------------------------

Для реализации качественного сервиса рекомендаций недостаточно просто отправить JSON в модель. Требуется использование продвинутых техник промпт-инжиниринга.

### 6.1 Паттерны промптинга

Для дипломной работы рекомендуется использовать комбинацию следующих подходов:

1.  **Chain-of-Thought (CoT):** Заставить модель "подумать" перед генерацией JSON. В промпте следует указать: "Сначала проанализируй интересы пользователя и погоду, составь краткий план рассуждений, и только затем генерируй финальный JSON". Это снижает вероятность логических ошибок (например, планирование ужина перед обедом).^19^

2.  **Role Prompting (Persona):** "Ты --- опытный локальный гид, который знает скрытые жемчужины города и оптимизирует логистику".

3.  **Structured Output Enforcement:** Использование параметра `response_format={"type": "json_object"}` в API OpenAI или использование библиотек типа `instructor` для принудительного соответствия схеме Pydantic.^13^

### 6.2 Обработка ошибок генерации (Self-Correction)

LLM не детерминированы. Возможны случаи, когда модель вернет невалидный JSON (например, пропустит обязательное поле). Стратегия реализации должна включать механизм **Self-Correction** (самокоррекции):

1.  Попытка валидации ответа через `TripPlan.model_validate_json(response)`.

2.  При возникновении `ValidationError`: Поймать ошибку.

3.  Сформировать новый запрос к LLM, включающий предыдущий неверный ответ и текст ошибки валидации: "Ты вернул неверный JSON. Ошибка: [текст ошибки]. Исправь это и верни валидный JSON".

4.  Повторить попытку (максимум 1-2 раза).

Этот паттерн значительно повышает надежность сервиса (Robustness).^20^

7\. Детальный план реализации (Step-by-Step)
--------------------------------------------

Данный план разбит на атомарные шаги, которые можно последовательно скармливать AI-ассистенту (например, Cursor, GitHub Copilot или ChatGPT) для генерации кода.

### Фаза 1: Инициализация и Базовая структура

**Шаг 1.1: Создание скелета проекта FastAPI**

-   **Задача:** Создать структуру папок, настроить `pyproject.toml` (Poetry), `Dockerfile` и базовый `app/main.py`.

-   **Требования:** Python 3.11+, FastAPI, Uvicorn. Настройка переменных окружения через `pydantic-settings` (`app/core/config.py`).

-   **Промпт для бота:** "Создай структуру проекта FastAPI для микросервиса AI Recommender. Используй Poetry. Настрой `config.py` для чтения `DATABASE_URL` и `OPENAI_API_KEY` из.env. Создай `main.py` с healthcheck эндпоинтом."

**Шаг 1.2: Настройка асинхронной БД и Моделей**

-   **Задача:** Настроить SQLAlchemy Async Engine и создать модель для таблицы `integration.ai_runs`.

-   **Требования:** Использовать `asyncpg`. Определить модель `AIRun` в `app/models/ai_runs.py` в точном соответствии со схемой (UUID, Enum, JSONB). Настроить Alembic для миграций.

-   **Промпт для бота:** "Настрой асинхронное подключение к PostgreSQL с SQLAlchemy 2.0. Создай модель `AIRun` для таблицы `integration.ai_runs` с полями id (uuid), prompt (text), response (jsonb), tokens_used (int), status (enum). Создай скрипт для инициализации миграций Alembic."

### Фаза 2: Контракты данных и Логика

**Шаг 2.1: Определение Pydantic схем (DTO)**

-   **Задача:** Реализовать схемы запросов и ответов.

-   **Требования:** Файл `app/schemas/itinerary.py`. Строгая типизация. Вложенные модели для дней и активностей.

-   **Промпт для бота:** "Создай Pydantic модели для генерации маршрута. Входная модель: профиль пользователя, бюджет, даты. Выходная модель: Маршрут, содержащий список дней, каждый день содержит список активностей с координатами и описанием. Используй Pydantic V2."

**Шаг 2.2: Реализация сервиса Телеметрии**

-   **Задача:** Создать логику сохранения логов в БД.

-   **Требования:** Функция `log_ai_interaction` в `app/services/telemetry.py`. Должна принимать сессию БД и данные, писать в БД асинхронно.

-   **Промпт для бота:** "Напиши асинхронный сервис для логирования запусков AI. Функция должна принимать данные промпта и ответа, и сохранять их в таблицу `ai_runs`. Подготовь её для использования в BackgroundTasks."

### Фаза 3: Интеграция с LLM и Внешними API

**Шаг 3.1: Клиент для Integration Service**

-   **Задача:** Реализовать HTTP-клиент для общения с сервисом погоды и карт.

-   **Требования:** `app/services/integration_client.py`. Использовать `httpx.AsyncClient`. Реализовать методы `get_weather` и `search_poi`. Добавить заглушки (mock) ответов для тестирования.

-   **Промпт для бота:** "Создай класс `IntegrationClient` на базе httpx. Реализуй методы получения погоды и поиска мест. Добавь обработку ошибок соединения и таймаутов."

**Шаг 3.2: Движок генерации (LLM Engine)**

-   **Задача:** Реализовать логику вызова OpenAI API.

-   **Требования:** `app/services/llm_engine.py`. Сборка системного промпта из шаблонов. Вызов API с `response_format='json_object'`. Валидация через Pydantic.

-   **Промпт для бота:** "Реализуй сервис для работы с OpenAI API. Метод `generate_itinerary` должен принимать контекст, формировать промпт, делать запрос к GPT-4o и возвращать валидированный Pydantic объект маршрута. Реализуй обработку ошибок JSON."

### Фаза 4: API Эндпоинты и Сборка

**Шаг 4.1: Реализация эндпоинта `/recommend`**

-   **Задача:** Собрать всё вместе в контроллере.

-   **Требования:** `app/api/v1/endpoints/recommendation.py`. Получить запрос -> Запросить погоду (Integration) -> Запросить LLM -> Запустить фоновую задачу логирования -> Вернуть ответ.

-   **Промпт для бота:** "Создай FastAPI эндпоинт `POST /internal/v1/ai/recommend`. Он должен принимать `RecommendationRequest`, вызывать IntegrationClient и LLMEngine, сохранять лог в BackgroundTasks и возвращать `ItineraryResponse`."

**Шаг 4.2: Реализация `/explain` и `/improve`**

-   **Задача:** Добавить эндпоинты для объяснения и улучшения.

-   **Требования:** Аналогичная логика, но с другими промптами. Для `/improve` на вход подается старый маршрут и запрос на изменение.

-   **Промпт для бота:** "Реализуй эндпоинты `/explain` (объяснение выбора) и `/improve` (изменение маршрута по запросу пользователя). Используй отдельные шаблоны промптов для этих задач."

**Шаг 4.3: Безопасность (Internal Auth)**

-   **Задача:** Защитить внутренние эндпоинты.

-   **Требования:** Простой механизм проверки токена (Dependency Injection), проверяющий наличие специального заголовка или JWT от Trip Service.

-   **Промпт для бота:** "Добавь зависимость (Dependency) для проверки секретного токена в заголовке запроса. Примени её ко всем внутренним эндпоинтам для защиты межсервисного взаимодействия."

8\. Стратегия тестирования
--------------------------

Для обеспечения качества необходимо использовать `pytest`. Тесты должны быть разделены на unit (тестирование логики промптов и валидации) и integration (тестирование эндпоинтов с моками внешних сервисов).

-   **Mocking:** При тестировании эндпоинтов критически важно "мокать" (подменять заглушками) вызовы к OpenAI и Integration Service, чтобы тесты были быстрыми, детерминированными и бесплатными. Используйте библиотеку `respx` или `unittest.mock` для перехвата HTTP-запросов от `httpx`.

9\. Операционная поддержка и мониторинг
---------------------------------------

После развертывания сервиса необходимо отслеживать ключевые метрики, используя данные из таблицы `ai_runs`.

| **Метрика** | **SQL-запрос (Концепт)** | **Цель анализа** |
| --- | --- | --- |
| **Token Cost per Trip** | `AVG(tokens_used)` | Оценка рентабельности и юнит-экономики. |
| **Error Rate** | `COUNT(status='failed') / COUNT(*)` | Выявление проблем с промптами или доступностью API провайдера. |
| **Latency** | `created_at` (start) vs `updated_at` (end) | Мониторинг SLO (цель < 1.5 сек для простых запросов). |

10\. Заключение
---------------

Предложенный план реализации представляет собой структурированную, технически обоснованную дорожную карту для создания микросервиса AI Recommender. Использование FastAPI и асинхронности обеспечивает необходимую производительность, Pydantic гарантирует целостность данных, а архитектура с разделением на слои (Layered Architecture) позволяет гибко менять логику промптинга без переписывания всего сервиса. Реализация данного плана позволит создать надежный, расширяемый и наблюдаемый сервис, полностью соответствующий требованиям дипломной работы уровня Software Engineering.